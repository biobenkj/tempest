# Custom Tempest Configuration for Complex Read Architecture with Probabilistic PWM
# Sequence structure: p7, i7, RP2, UMI, ACC, cDNA, polyA, CBC, RP1, i5, p5

# Model Architecture Configuration
model:
  max_seq_len: 1500  # Increased for complex architecture
  num_labels: 11     # p7, i7, RP2, UMI, ACC, cDNA, polyA, CBC, RP1, i5, p5
  embedding_dim: 128
  lstm_units: 256
  lstm_layers: 2
  dropout: 0.3
  use_cnn: true
  use_bilstm: true
  batch_size: 32
  vocab_size: 5  # A, C, G, T, N
  cnn_filters: [64, 128]
  cnn_kernels: [3, 5]
  use_crf: true  # Add this for CRF layer

# Data Simulation Configuration
simulation:
  # Number of sequences to generate
  num_sequences: 50000
  train_split: 0.8

  # Introduce invalid reads
  invalid_fraction: 0.1
  invalid_types: ["segment_loss", "segment_duplication", "truncation", "chimeric", "scrambled"]
  invalid_params:
    segment_loss_prob: 0.2
    segment_dup_prob: 0.2
    truncated_prob: 0.2
    chimeric_prob: 0.2
    scrambled_prob: 0.2
  
  # Additional test/validation splits
  n_train: 40000
  n_val: 5000
  n_test: 5000
  
  # Random seed for reproducibility
  random_seed: 42
  
  # Sequence architecture - your specific read structure
  sequence_order:
    - p7
    - i7
    - RP2
    - UMI
    - ACC
    - cDNA
    - polyA
    - CBC
    - RP1
    - i5
    - p5
  
  # Enable full read reverse complement to simulate bidirectional sequencing
  full_read_reverse_complement_prob: 0.5
  
  # Probability that a read will have errors injected (when errors.enabled = true)
  error_injection_prob: 0.1
  
  # Fixed sequences for adapters and primers
  sequences:
    p7: "CAAGCAGAAGACGGCATACGAGAT"
    RP2: "GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT"
    RP1: "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"
    p5: "GTGTAGATCTCGGTGGTCGCCGTATCATT"
    UMI: "random"
    cDNA: "transcript"
    polyA: "polya"
  
  # Whitelist files
  whitelist_files:
    i7: "/varidata/research/projects/shen/tools/benkj/tempest/whitelist/udi_i7.txt"
    i5: "/varidata/research/projects/shen/tools/benkj/tempest/whitelist/udi_i5.txt"
    CBC: "/varidata/research/projects/shen/tools/benkj/tempest/whitelist/cbc.txt"
  
  # Whitelist reverse complement configuration
  # Set to true for segments that should be reverse complemented
  # Common setup: i7 is typically RC'd in Illumina sequencing, i5 and CBC are forward
  # Adjust based on your specific sequencing chemistry
  whitelist_rc:
    i7: true      # Reverse complement i7 indices
    i5: false     # Keep i5 indices forward (change to true if your chemistry requires RC)
    CBC: false    # Keep cell barcodes forward
  
  # PWM configuration for ACC - USING PROBABILISTIC APPROACH
  pwm_files:
    ACC: "/varidata/research/projects/shen/tools/benkj/tempest/whitelist/acc_pwm.txt"
  
  # Probabilistic PWM parameters for ACC generation
  pwm:
    pwm_file: "/varidata/research/projects/shen/tools/benkj/tempest/whitelist/acc_pwm.txt"
    temperature: 1.2
    min_entropy: 0.1
    diversity_boost: 1.0
    pattern: "ACCSSV"
    random_seed: 42  # Important for reproducibility - should match simulation.random_seed
  
  # Segment generation parameters
  segment_generation:
    lengths:
      p7: 24
      i7: 8
      RP2: 34
      UMI: 8
      ACC: 6
      cDNA: 500
      polyA: 30
      CBC: 6
      RP1: 33
      i5: 8
      p5: 29
    
    generation_mode:
      p7: "fixed"
      i7: "whitelist"
      RP2: "fixed"
      UMI: "random"
      ACC: "pwm"
      cDNA: "transcript"
      polyA: "polya"
      CBC: "whitelist"
      RP1: "fixed"
      i5: "whitelist"
      p5: "fixed"
  
  # Segment length ranges
  sequence_lengths:
    p7: {min_length: 24, max_length: 24}
    i7: {min_length: 8, max_length: 8}
    RP2: {min_length: 34, max_length: 34}
    UMI: {min_length: 8, max_length: 8}
    ACC: {min_length: 6, max_length: 6}
    cDNA: {min_length: 200, max_length: 1000}
    polyA: {min_length: 10, max_length: 50}
    CBC: {min_length: 6, max_length: 6}
    RP1: {min_length: 33, max_length: 33}
    i5: {min_length: 8, max_length: 8}
    p5: {min_length: 29, max_length: 29}
  
  # Transcript configuration for cDNA segments
  transcript:
    fasta_file: "gencode.v49.transcripts.fa.gz"
    min_length: 500
    max_length: 2200
    fragment_mode: true
    fragment_min: 200
    fragment_max: 1000
    trim_polya: true  # Remove polyA tails from transcripts before fragment sampling
                      # When enabled, metadata will include original_length, trimmed_length, and polya_trimmed
    reverse_complement_prob: 0.5  # Probability for individual fragments
    decompress_to_tmp: true
    tmpdir: "./tmp"
    cache_size_mb: 512
    fallback_gc_content: 0.5  # GC content for fallback random sequences
    fallback_mode: "random"  # What to do if transcripts fail to load
  
  # PolyA tail configuration (FIXED NAME FROM polya_tail)
  polya:
    min_length: 10
    max_length: 50
    purity: 0.95
    distribution: "normal"
    mean_length: 30  # Changed from 'mean' to 'mean_length' for clarity
    std_length: 10   # Changed from 'std' to 'std_length' for clarity
    absolute_min: 20  # Hard minimum boundary
    absolute_max: 500 # Hard maximum boundary
  
  # Error simulation parameters (FIXED NAME FROM error_injection)
  errors:
    enabled: true
    substitution_rate: 0.001
    insertion_rate: 0.0001
    deletion_rate: 0.0001
    quality_dependent: false  # Whether errors depend on quality scores

# Training Configuration
training:
  epochs: 50
  batch_size: 32
  learning_rate: 0.001
  optimizer: "adam"
  train_split: 0.8  # Add this since simulation references it
  
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.0001
    monitor: "val_loss"
    restore_best: true
  
  use_class_weights: true
  
  checkpoint:
    enabled: true
    save_best_only: true
    monitor: "val_loss"
    save_freq: "epoch"
  
  tensorboard:
    enabled: true
    log_dir: "./logs"
    histogram_freq: 0
    write_graph: true
    update_freq: "epoch"
  
  # Learning rate scheduler
  lr_scheduler:
    enabled: true
    type: "reduce_on_plateau"
    factor: 0.5
    patience: 5
    min_lr: 0.00001

# Hybrid Training Configuration
hybrid:
  enabled: true
  
  # Training phases for hybrid models
  warmup_epochs: 10        # Phase 1: Standard training
  discriminator_epochs: 10 # Phase 2: Adversarial with invalid reads
  pseudolabel_epochs: 10   # Phase 3: Pseudo-labeling with unlabeled data
  
  # Invalid read generation for robustness
  invalid_generation:
    enabled: true
    invalid_fraction: 0.2
    augmentation_prob: 0.3
  
  # Pseudo-labeling configuration
  pseudolabel:
    enabled: true
    confidence_threshold: 0.9
    max_samples: 10000
    update_frequency: 5  # Update pseudo-labels every N epochs
  
  # Discriminator configuration for adversarial training
  discriminator:
    enabled: true
    architecture: "simple"  # 'simple' or 'complex'
    learning_rate: 0.001
    weight: 0.5  # Weight of discriminator loss
  
  # Constraint configurations
  length_constraints:
    enabled: true
    enforce_during_training: true
    enforce_during_inference: true
    segments:
      p7: [24, 24]
      i7: [8, 8]
      RP2: [34, 34]
      UMI: [8, 8]
      ACC: [6, 6]
      CBC: [6, 6]
      RP1: [33, 33]
      i5: [8, 8]
      p5: [29, 29]
  
  whitelist_constraints:
    enabled: true
    segments: ["i7", "i5", "CBC"]
    enforce_during_training: true
    enforce_during_inference: true
  
  pwm_constraints:
    enabled: true
    segments: ["ACC"]
    use_probabilistic_scoring: true
    scoring_method: "log_likelihood"
    min_score: -10.0
    score_weight: 0.5

# Enhanced Ensemble Configuration
ensemble:
  enabled: true
  num_models: 5  # Total number of models in ensemble
  
  # Voting/combination method
  voting_method: "bayesian_model_averaging"  # or 'weighted_average', 'voting', 'stacking'
  
  # Model type configuration
  hybrid_ratio: 0.4  # 40% hybrid models, 60% standard models
  
  # Variation configuration
  variation_type: "both"  # 'architecture', 'initialization', or 'both'
  
  # Architecture variations for diversity
  architecture_variations:
    vary_lstm_units: [128, 256, 512]
    vary_lstm_layers: [2, 3]
    vary_dropout: [0.2, 0.3, 0.4, 0.5]
    vary_embedding_dim: [64, 128, 256]
    vary_cnn_filters: [[64, 128], [128, 256], [32, 64, 128]]
  
  # Initialization variations
  random_seeds: [42, 123, 456, 789, 1011]
  
  # Comprehensive BMA Configuration
  bma_config:
    # Enable/disable BMA
    enabled: true
    
    # Prior type: 'uniform', 'informative', 'adaptive'
    prior_type: "adaptive"
    
    # Prior weights (optional, for 'informative' prior)
    prior_weights: null
    
    # Approximation method: 'bic', 'laplace', 'variational', 'cross_validation'
    approximation: "bic"
    
    # Approximation parameters for different methods
    approximation_params:
      bic:
        penalty_factor: 1.0  # Standard BIC penalty
      
      laplace:
        num_samples: 1000    # Number of samples for Laplace approximation
        damping: 0.01        # Damping factor for numerical stability
      
      variational:
        num_iterations: 100  # VI iterations
        learning_rate: 0.01  # VI learning rate
        convergence_threshold: 1e-4
      
      cross_validation:
        num_folds: 5         # K-fold CV
        stratified: true     # Use stratified splitting
    
    # Temperature for posterior computation
    temperature: 1.0
    
    # Compute posterior variance for uncertainty quantification
    compute_posterior_variance: true
    
    # Normalize posterior weights to sum to 1
    normalize_posteriors: true
    
    # Minimum posterior weight threshold
    min_posterior_weight: 0.01
    
    # Model selection criteria (optional)
    selection_criteria:
      metric: "validation_accuracy"
      threshold: 0.05  # Evidence threshold for model selection
    
    # Use model averaging vs. model selection
    use_model_averaging: true
    
    # Evidence threshold for inclusion
    evidence_threshold: 0.05
  
  # Weighted average configuration (alternative to BMA)
  weighted_average_config:
    optimization: "performance"  # 'fixed', 'performance', 'learned'
    
    # Fixed weights (if using 'fixed' optimization)
    fixed_weights: null
    
    # Performance-based weighting
    performance_metrics:
      val_accuracy: 0.7
      val_loss: 0.3
    
    # Type-based bonus weights
    type_bonus:
      hybrid: 1.1
      standard: 1.0
  
  # Prediction aggregation
  prediction_aggregation:
    method: "weighted_average"  # 'weighted_average', 'voting', 'stacking'
    confidence_weighting: true
    uncertainty_estimation: true
    apply_temperature_scaling: false
    temperature: 1.0
  
  # Calibration for ensemble predictions
  calibration:
    enabled: true
    method: "isotonic"  # 'isotonic', 'platt', 'temperature_scaling'
    use_separate_calibration_set: true
    calibration_split: 0.2  # Use 20% of validation for calibration
  
  # Diversity metrics and enforcement
  diversity:
    enforce_diversity: true
    diversity_metric: "disagreement"  # 'disagreement', 'correlation', 'kl_divergence'
    min_diversity_threshold: 0.1
  
  # Ensemble evaluation
  evaluation:
    compute_individual_metrics: true
    compute_ensemble_metrics: true
    metrics: ["accuracy", "f1", "segment_accuracy", "calibration_error"]
    analyze_model_contributions: true
    
    # Model selection criteria (if you want to select subset)
    selection:
      enabled: false
      method: "greedy"
      metric: "val_accuracy"
      max_models: 3
  
  # Uncertainty quantification
  uncertainty:
    compute_epistemic: true  # Model uncertainty
    compute_aleatoric: true  # Data uncertainty
    method: "ensemble_variance"
    confidence_intervals: true
    interval_alpha: 0.95
  
  # Save configuration
  save:
    save_all_models: true
    save_ensemble_metadata: true
    save_predictions: true
    checkpoint_frequency: 5

# Evaluation Configuration
evaluation:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "segment_accuracy"
    - "edit_distance"
    - "boundary_accuracy"
    - "acc_diversity"
    - "acc_score_distribution"
  
  per_segment_metrics: true
  
  confusion_matrix:
    enabled: true
    normalize: true
  
  error_analysis:
    enabled: true
    sample_errors: 100
    save_errors: true
  
  acc_evaluation:
    enabled: true
    compute_entropy: true
    compute_divergence: true
    save_acc_sequences: true

# Visualization Configuration
visualization:
  enabled: true
  plots:
    - "training_curves"
    - "confusion_matrix"
    - "segment_performance"
    - "length_distribution"
    - "error_patterns"
    - "model_comparison"
    - "ensemble_weights"
    - "model_diversity"
    - "acc_pwm_heatmap"
    - "acc_sequence_logo"
    - "acc_score_histogram"
  
  save_plots: true
  plot_dir: "./plots"

# Demultiplexing Configuration
demux:
  # Sequence architecture - your specific read structure
  sequence_order:
    - p7
    - i7
    - RP2
    - UMI
    - ACC
    - cDNA
    - polyA
    - CBC
    - RP1
    - i5
    - p5
  
  # Model path
  model_path: "/path/to/model.h5"

  # Set the minimum edit distance allowed
  edit_distance:
    i7: 2
    i5: 2
    CBC: 1
  
  # Whitelist files
  whitelist_files:
    i7: "/varidata/research/projects/shen/tools/benkj/tempest/whitelist/udi_i7.txt"
    i5: "/varidata/research/projects/shen/tools/benkj/tempest/whitelist/udi_i5.txt"
    CBC: "/varidata/research/projects/shen/tools/benkj/tempest/whitelist/cbc.txt"
  
  # Sample file
  sample_file: "/path/to/sample_file.txt"

  # How to save the files
  save_fastq: true
  save_fasta: false
  output_dir: "/path/to/demux/dir"

# Logging Configuration
logging:
  level: "INFO"
  log_to_file: true
  log_file: "./tempest_training.log"
  log_whitelist_usage: true
  log_generation_stats: true
  log_pwm_stats: true
  log_ensemble_progress: true

# Output Configuration
output:
  save_dir: "./tempest_output"
  save_predictions: true
  save_model: true
  save_config: true
  create_report: true
  save_acc_analysis: true
  save_ensemble_analysis: true